{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T14:10:48.304176Z",
     "start_time": "2025-02-12T14:10:47.045519Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:11:06.177650Z",
     "start_time": "2025-02-12T14:10:53.708280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of NHANES cycles to download data from (you can update this as needed)\n",
    "years = ['2015-2016', '2017-2018']  # Add more cycles if needed\n",
    "\n",
    "# Base URL for NHANES data\n",
    "base_url = \"https://wwwn.cdc.gov/nchs/nhanes/{year}/\"\n",
    "\n",
    "# List of datasets to download for each cycle (you can add more if necessary)\n",
    "datasets = {\n",
    "    'DEMO': 'DEMO_G.csv',  # Demographics\n",
    "    'DR1TOTX': 'DR1TOTX.csv',  # Dietary (First Recall)\n",
    "    'DR2TOTX': 'DR2TOTX.csv',  # Dietary (Second Recall)\n",
    "    'LAB': 'LAB_G.csv',  # Laboratory\n",
    "    'EXAM': 'EXAM_G.csv',  # Physical Exam\n",
    "    'RXQ': 'RXQ_G.csv',  # Medications\n",
    "}\n",
    "\n",
    "# Function to download files\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}.\")\n",
    "\n",
    "# Download the data for each year and dataset\n",
    "def download_nhanes_data(years, datasets):\n",
    "    if not os.path.exists('nhanes_data'):\n",
    "        os.makedirs('nhanes_data')\n",
    "\n",
    "    for year in years:\n",
    "        for dataset, filename in datasets.items():\n",
    "            url = base_url.format(year=year) + dataset\n",
    "            download_file(url, os.path.join('nhanes_data', filename))\n",
    "\n",
    "# Function to load and merge the data for a specific cycle\n",
    "def load_and_merge_data(years, datasets):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for year in years:\n",
    "        year_data = []\n",
    "        \n",
    "        for dataset, filename in datasets.items():\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join('nhanes_data', filename)\n",
    "            \n",
    "            # Check if the file exists, and if so, read it\n",
    "            if os.path.exists(file_path):\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Merge data for the year\n",
    "                if merged_df.empty:\n",
    "                    merged_df = df\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df, on='SEQN', how='outer')\n",
    "                    \n",
    "        year_filename = f\"merged_nhanes_{year}.csv\"\n",
    "        merged_df.to_csv(os.path.join('nhanes_data', year_filename), index=False)\n",
    "        print(f\"Data for {year} merged and saved as {year_filename}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Step 1: Download the data files for each year\n",
    "download_nhanes_data(years, datasets)\n",
    "\n",
    "# Step 2: Load and merge the data across years\n",
    "merged_data = load_and_merge_data(years, datasets)\n",
    "\n",
    "# Step 3: Inspect the merged data\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(merged_data.head())\n",
    "\n",
    "# Optionally save the final merged dataset (across all years)\n",
    "merged_data.to_csv('merged_nhanes_all_years.csv', index=False)\n"
   ],
   "id": "21228d83eb56d7b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: nhanes_data\\DEMO_G.csv\n",
      "Downloaded: nhanes_data\\DR1TOTX.csv\n",
      "Downloaded: nhanes_data\\DR2TOTX.csv\n",
      "Downloaded: nhanes_data\\LAB_G.csv\n",
      "Downloaded: nhanes_data\\EXAM_G.csv\n",
      "Downloaded: nhanes_data\\RXQ_G.csv\n",
      "Downloaded: nhanes_data\\DEMO_G.csv\n",
      "Downloaded: nhanes_data\\DR1TOTX.csv\n",
      "Downloaded: nhanes_data\\DR2TOTX.csv\n",
      "Downloaded: nhanes_data\\LAB_G.csv\n",
      "Downloaded: nhanes_data\\EXAM_G.csv\n",
      "Downloaded: nhanes_data\\RXQ_G.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 10, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 69\u001B[0m\n\u001B[0;32m     66\u001B[0m download_nhanes_data(years, datasets)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Step 2: Load and merge the data across years\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m \u001B[43mload_and_merge_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43myears\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatasets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# Step 3: Inspect the merged data\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMerged data shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmerged_data\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 51\u001B[0m, in \u001B[0;36mload_and_merge_data\u001B[1;34m(years, datasets)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Check if the file exists, and if so, read it\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(file_path):\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;66;03m# Read the CSV file into a DataFrame\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# Merge data for the year\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m merged_df\u001B[38;5;241m.\u001B[39mempty:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2061\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 1 fields in line 10, saw 3\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T14:09:10.232103Z",
     "start_time": "2025-02-11T14:09:05.054878Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pyreadstat",
   "id": "44134e2a0dfe8879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Downloading pyreadstat-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyreadstat) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rampa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.8-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB 325.1 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/2.4 MB 871.5 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.4 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:12:15.054795Z",
     "start_time": "2025-02-12T14:11:13.470397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def load_nhanes_data(url):\n",
    "    \"\"\"Download and load an NHANES dataset from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error if the download failed\n",
    "    return pd.read_sas(BytesIO(response.content), format='xport')\n",
    "\n",
    "# Define the URLs for each dataset for three cycles\n",
    "cycles = {\n",
    "    \"2017\": {  # 2017-2018 cycle\n",
    "        \"dr1iff\": \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DR1IFF_J.xpt\",\n",
    "        \"dr1tot\": \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DR1TOT_J.XPT\",\n",
    "        \"diq\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DIQ_J.xpt\",\n",
    "        \"glu\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/GLU_J.xpt\",\n",
    "        \"bmx\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/BMX_J.xpt\",\n",
    "        \"demo\":   \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\"\n",
    "    },\n",
    "    \"2015\": {  # 2015-2016 cycle\n",
    "        \"dr1iff\": \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DR1IFF_I.xpt\",\n",
    "        \"dr1tot\": \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DR1TOT_I.XPT\",\n",
    "        \"diq\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DIQ_I.xpt\",\n",
    "        \"glu\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/GLU_I.xpt\",\n",
    "        \"bmx\":    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/BMX_I.xpt\",\n",
    "        \"demo\":   \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\"\n",
    "    },\n",
    "}\n",
    "\n",
    "merged_cycles = []\n",
    "\n",
    "# Process each cycle separately\n",
    "for cycle, url_dict in cycles.items():\n",
    "    print(f\"Processing NHANES cycle: {cycle}\")\n",
    "    \n",
    "    # Download and subset useful columns for each dataset\n",
    "    demo = load_nhanes_data(url_dict[\"demo\"])[[\"SEQN\", \"RIDAGEYR\", \"RIAGENDR\", \"RIDRETH1\"]]\n",
    "    dr1tot = load_nhanes_data(url_dict[\"dr1tot\"])[[\"SEQN\", \"DR1TKCAL\", \"DR1TPROT\", \"DR1TCARB\", \"DR1TTFAT\", \"DR1TSFAT\"]]\n",
    "    dr1iff = load_nhanes_data(url_dict[\"dr1iff\"])[[\"SEQN\", \"DR1IFDCD\"]]\n",
    "    diq = load_nhanes_data(url_dict[\"diq\"])[[\"SEQN\", \"DIQ010\"]]\n",
    "    glu = load_nhanes_data(url_dict[\"glu\"])[[\"SEQN\", \"LBXGLU\"]]\n",
    "    bmx = load_nhanes_data(url_dict[\"bmx\"])[[\"SEQN\", \"BMXWT\", \"BMXHT\", \"BMXBMI\"]]\n",
    "    \n",
    "    # Rename columns to user-friendly names\n",
    "    demo = demo.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"RIDAGEYR\": \"Age\",\n",
    "        \"RIAGENDR\": \"Gender\",         # 1 = Male, 2 = Female\n",
    "        \"RIDRETH1\": \"RaceEthnicity\"\n",
    "    })\n",
    "    \n",
    "    dr1tot = dr1tot.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"DR1TKCAL\": \"TotalCalories\",\n",
    "        \"DR1TPROT\": \"Protein\",\n",
    "        \"DR1TCARB\": \"Carbohydrates\",\n",
    "        \"DR1TTFAT\": \"TotalFat\",\n",
    "        \"DR1TSFAT\": \"SaturatedFat\"\n",
    "    })\n",
    "    \n",
    "    dr1iff = dr1iff.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"DR1IFDCD\": \"FoodCode\"\n",
    "    })\n",
    "    \n",
    "    diq = diq.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"DIQ010\": \"DiabetesDiagnosis\"  # Typically: 1 = Yes, 2 = No\n",
    "    })\n",
    "    \n",
    "    glu = glu.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"LBXGLU\": \"BloodGlucose\"\n",
    "    })\n",
    "    \n",
    "    bmx = bmx.rename(columns={\n",
    "        \"SEQN\": \"ParticipantID\",\n",
    "        \"BMXWT\": \"Weight\",\n",
    "        \"BMXHT\": \"Height\",\n",
    "        \"BMXBMI\": \"BMI\"\n",
    "    })\n",
    "    \n",
    "    # Merge datasets for this cycle on ParticipantID\n",
    "    merged_df = demo.merge(dr1tot, on=\"ParticipantID\", how=\"inner\") \\\n",
    "                    .merge(dr1iff, on=\"ParticipantID\", how=\"inner\") \\\n",
    "                    .merge(diq, on=\"ParticipantID\", how=\"inner\") \\\n",
    "                    .merge(glu, on=\"ParticipantID\", how=\"inner\") \\\n",
    "                    .merge(bmx, on=\"ParticipantID\", how=\"inner\")\n",
    "    \n",
    "    # Add a column to indicate the NHANES cycle\n",
    "    merged_df[\"Cycle\"] = cycle\n",
    "    merged_cycles.append(merged_df)\n",
    "\n",
    "# Concatenate data from all cycles into a single DataFrame\n",
    "combined_df = pd.concat(merged_cycles, ignore_index=True)\n",
    "\n",
    "# Save the combined dataset to CSV\n",
    "combined_df.to_csv(\"combined_nhanes_data.csv\", index=False)\n",
    "print(\"Combined dataset saved to 'combined_nhanes_data.csv'\")\n"
   ],
   "id": "a1d714757c9e7682",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NHANES cycle: 2017\n",
      "Processing NHANES cycle: 2015\n",
      "Combined dataset saved to 'combined_nhanes_data.csv'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:12:20.220797Z",
     "start_time": "2025-02-12T14:12:20.213460Z"
    }
   },
   "cell_type": "code",
   "source": "combined_df.shape",
   "id": "ebac22da242ad3c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84030, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:12:21.729628Z",
     "start_time": "2025-02-12T14:12:21.708392Z"
    }
   },
   "cell_type": "code",
   "source": "combined_df.head()",
   "id": "b7680011f05ce4cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   ParticipantID   Age  Gender  RaceEthnicity  TotalCalories  Protein  \\\n",
       "0        93708.0  66.0     2.0            5.0         1251.0    50.96   \n",
       "1        93708.0  66.0     2.0            5.0         1251.0    50.96   \n",
       "2        93708.0  66.0     2.0            5.0         1251.0    50.96   \n",
       "3        93708.0  66.0     2.0            5.0         1251.0    50.96   \n",
       "4        93708.0  66.0     2.0            5.0         1251.0    50.96   \n",
       "\n",
       "   Carbohydrates  TotalFat  SaturatedFat    FoodCode  DiabetesDiagnosis  \\\n",
       "0         123.71     65.49        17.446  53233060.0                3.0   \n",
       "1         123.71     65.49        17.446  94100100.0                3.0   \n",
       "2         123.71     65.49        17.446  54403054.0                3.0   \n",
       "3         123.71     65.49        17.446  92101000.0                3.0   \n",
       "4         123.71     65.49        17.446  11100000.0                3.0   \n",
       "\n",
       "   BloodGlucose  Weight  Height   BMI Cycle  \n",
       "0         122.0    53.5   150.2  23.7  2017  \n",
       "1         122.0    53.5   150.2  23.7  2017  \n",
       "2         122.0    53.5   150.2  23.7  2017  \n",
       "3         122.0    53.5   150.2  23.7  2017  \n",
       "4         122.0    53.5   150.2  23.7  2017  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParticipantID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>RaceEthnicity</th>\n",
       "      <th>TotalCalories</th>\n",
       "      <th>Protein</th>\n",
       "      <th>Carbohydrates</th>\n",
       "      <th>TotalFat</th>\n",
       "      <th>SaturatedFat</th>\n",
       "      <th>FoodCode</th>\n",
       "      <th>DiabetesDiagnosis</th>\n",
       "      <th>BloodGlucose</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Cycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93708.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>123.71</td>\n",
       "      <td>65.49</td>\n",
       "      <td>17.446</td>\n",
       "      <td>53233060.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>150.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93708.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>123.71</td>\n",
       "      <td>65.49</td>\n",
       "      <td>17.446</td>\n",
       "      <td>94100100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>150.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93708.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>123.71</td>\n",
       "      <td>65.49</td>\n",
       "      <td>17.446</td>\n",
       "      <td>54403054.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>150.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93708.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>123.71</td>\n",
       "      <td>65.49</td>\n",
       "      <td>17.446</td>\n",
       "      <td>92101000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>150.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93708.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>123.71</td>\n",
       "      <td>65.49</td>\n",
       "      <td>17.446</td>\n",
       "      <td>11100000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>150.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:12:25.696925Z",
     "start_time": "2025-02-12T14:12:25.689694Z"
    }
   },
   "cell_type": "code",
   "source": "combined_df.ParticipantID.nunique()",
   "id": "14aaad9bfda8c8eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5707"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:12:28.407027Z",
     "start_time": "2025-02-12T14:12:28.401761Z"
    }
   },
   "cell_type": "code",
   "source": "combined_df.columns",
   "id": "f66e49910183c19d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ParticipantID', 'Age', 'Gender', 'RaceEthnicity', 'TotalCalories',\n",
       "       'Protein', 'Carbohydrates', 'TotalFat', 'SaturatedFat', 'FoodCode',\n",
       "       'DiabetesDiagnosis', 'BloodGlucose', 'Weight', 'Height', 'BMI',\n",
       "       'Cycle'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e23c291ac25f340f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
